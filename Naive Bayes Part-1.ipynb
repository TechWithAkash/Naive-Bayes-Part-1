{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71f130d7",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Q1. What is Bayes' theorem?\n",
    "Bayes' theorem is a fundamental principle in probability theory used to update or revise the probability for a hypothesis (or an event) based on new evidence or information. It describes the probability of an event, given prior knowledge or conditions related to the event.\n",
    "\n",
    "### Q2. What is the formula for Bayes' theorem?\n",
    "The formula for Bayes' theorem can be expressed as:\n",
    "\n",
    "\\[ P(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)} \\]\n",
    "\n",
    "Where:\n",
    "- \\( P(A|B) \\) is the probability of event A occurring given that event B has occurred.\n",
    "- \\( P(B|A) \\) is the probability of event B occurring given that event A has occurred.\n",
    "- \\( P(A) \\) and \\( P(B) \\) are the probabilities of events A and B occurring, respectively.\n",
    "\n",
    "### Q3. How is Bayes' theorem used in practice?\n",
    "Bayes' theorem is applied in various fields, including statistics, machine learning, medical diagnosis, and information retrieval systems like spam filters. It's used to update prior probabilities based on new evidence, which helps in making better-informed decisions or predictions.\n",
    "\n",
    "### Q4. What is the relationship between Bayes' theorem and conditional probability?\n",
    "Bayes' theorem is derived from conditional probability. It provides a way to calculate conditional probabilities when probabilities of related events are known.\n",
    "\n",
    "### Q5. How do you choose which type of Naive Bayes classifier to use for any given problem?\n",
    "The choice of Naive Bayes classifier (Gaussian, Multinomial, or Bernoulli) depends on the nature of the features and assumptions about the underlying data distribution:\n",
    "- **Gaussian Naive Bayes** assumes continuous features that follow a Gaussian distribution.\n",
    "- **Multinomial Naive Bayes** is suitable for discrete counts (e.g., text classification with word counts).\n",
    "- **Bernoulli Naive Bayes** is effective for binary features (presence or absence of a feature).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa1bfb8",
   "metadata": {},
   "source": [
    "\n",
    "### Q6. Naive Bayes Classification Assignment:\n",
    "Using the provided dataset:\n",
    "- For class A: \\( P(A) = \\frac{3 + 3 + 4 + 4 + 3 + 3 + 3}{7 \\times 4} = \\frac{23}{28} \\)\n",
    "- For class B: \\( P(B) = \\frac{2 + 2 + 1 + 2 + 2 + 2 + 3}{7 \\times 4} = \\frac{14}{28} \\)\n",
    "\n",
    "For features X1 = 3 and X2 = 4:\n",
    "- \\( P(X1 = 3 | A) = \\frac{4}{7} \\) (Class A has 4 occurrences where X1 = 3)\n",
    "- \\( P(X2 = 4 | A) = \\frac{3}{7} \\) (Class A has 3 occurrences where X2 = 4)\n",
    "- \\( P(X1 = 3 | B) = \\frac{1}{7} \\) (Class B has 1 occurrence where X1 = 3)\n",
    "- \\( P(X2 = 4 | B) = \\frac{3}{7} \\) (Class B has 3 occurrences where X2 = 4)\n",
    "\n",
    "Apply Naive Bayes' theorem:\n",
    "- \\( P(A | X1 = 3, X2 = 4) = \\frac{P(X1 = 3 | A) \\cdot P(X2 = 4 | A) \\cdot P(A)}{P(X1 = 3) \\cdot P(X2 = 4)} \\)\n",
    "- \\( P(B | X1 = 3, X2 = 4) = \\frac{P(X1 = 3 | B) \\cdot P(X2 = 4 | B) \\cdot P(B)}{P(X1 = 3) \\cdot P(X2 = 4)} \\)\n",
    "\n",
    "Now calculate both probabilities and predict the class based on which probability is higher.\n",
    "\n",
    "\n",
    "To predict the class using Naive Bayes, we need to calculate the conditional probabilities of each class given the feature values X1=3 and X2=4 using the provided frequency table.\n",
    "\n",
    "For Naive Bayes, the prediction is based on the likelihood of each class given the observed feature values and the prior probabilities of each class. The class with the highest posterior probability given the observed features will be predicted.\n",
    "\n",
    "Let's calculate the probabilities step by step:\n",
    "\n",
    "Given:\n",
    "- P(Class = A) = P(Class = B) = 0.5 (equal prior probabilities for each class)\n",
    "\n",
    "For Class A:\n",
    "- P(X1=3|A) = 4/10\n",
    "- P(X2=4|A) = 3/10\n",
    "\n",
    "For Class B:\n",
    "- P(X1=3|B) = 1/7\n",
    "- P(X2=4|B) = 3/7\n",
    "\n",
    "Now, let's calculate the posterior probabilities using Bayes' theorem:\n",
    "\n",
    "For Class A:\n",
    "P(A|X1=3, X2=4) ∝ P(X1=3|A) * P(X2=4|A) * P(A) = (4/10) * (3/10) * 0.5 = 0.06\n",
    "\n",
    "For Class B:\n",
    "P(B|X1=3, X2=4) ∝ P(X1=3|B) * P(X2=4|B) * P(B) = (1/7) * (3/7) * 0.5 = 0.022\n",
    "\n",
    "Since P(A|X1=3, X2=4) > P(B|X1=3, X2=4), Naive Bayes would predict the new instance to belong to **Class A**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5666f0a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
